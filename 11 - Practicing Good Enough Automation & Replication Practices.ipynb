{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicing Good Enough Automation & Replication Practices\n",
    "\n",
    "Building on our [Automation and Benchmarking](08%20-%20Automation%20and%20Benchmarking.ipynb) notebook, we will introduce some best practices to help us improve the reliability, scalability, performance, and visibility of our digital research and development cycle. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Software\n",
    "\n",
    "\n",
    "> Back up (almost) everything created by a human being as soon as it is created   \n",
    "\n",
    "```bash\n",
    "notifications-addupdate -U \"https://public.\n",
    "```\n",
    "\n",
    "> Store each project in a folder that is mirrored off the researcher's working machine     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Versioning  \n",
    "\n",
    "\n",
    "#### Add a file called `CHANGELOG.txt` to the project's `docs` subfolder\n",
    "\n",
    "\n",
    "#### Copy the entire project whenever a significant change has been made      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version Control Systems  \n",
    "\n",
    "\n",
    "#### Use a version control system\n",
    "\n",
    "\n",
    "#### What not to put under version control     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What We Left Out  \n",
    "\n",
    "\n",
    "#### Build Tools\n",
    "\n",
    "\n",
    "#### Unit Tests  \n",
    "\n",
    "\n",
    "#### Continuous Integration  \n",
    "\n",
    "\n",
    "#### Unit Tests  \n",
    "\n",
    "\n",
    "#### Profiling and Performance Tuning  \n",
    "\n",
    "\n",
    "#### Documentation  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Everything as Code\n",
    "* Every piece of config, infrastructure, and software should be stored in VCS\n",
    "* Single source of truth\n",
    "* Reproducibility, versioning, change management\n",
    "* Avoid config drift\n",
    "* Everything is managed with the same practice and rigor as application code\n",
    "\n",
    "## Managing third-party code\n",
    "* What if you don't control 100% of your code?\n",
    "* Open science is built on open-source software\n",
    "  * This means a lot of third-party dependencies that you don't have direct control of.\n",
    "  * You can't fork everything, as it becomes a maintenance hassle and makes it difficult to contribute back upstream.\n",
    "* **Solution:** Submodules, explicit version requirements, meaningful tagging schemes, and bringing it all together in a single application repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Branching, Versioning, Tagging Strategies\n",
    "\n",
    "### GitFlow\n",
    "* [GitFlow](https://datasift.github.io/gitflow/IntroducingGitFlow.html), a successful git branching workflow for teams of any size.\n",
    "\n",
    "![The GitFlow Strategy](https://datasift.github.io/gitflow/GitFlowHotfixBranch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategies for Maintaining Credentials and Config\n",
    "* Travis CI encrypted values\n",
    "* Jenkins Freestyle job, injecting config as environment variables known only to build host\n",
    "* Consider your audience. How is your code meant to be used?\n",
    "  * Is it OSS?\n",
    "  * Do you want to encourage reuse?\n",
    "  * Will it be stored publicly or privately?\n",
    "  * Where will it be built?\n",
    "* **Admin Consideration:** Vault, secrets stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Delivery and Deployment\n",
    "* Once your build pipeline has run, and your changeset has passed all testing guards, what do you do?\n",
    "* Continuous Delivery:\n",
    "  * Package and publish your code\n",
    "  * Singularity Registry, DockerHub, etc..\n",
    "* Continuous Deployment:\n",
    "  * Package your code and deploy it to pre-prod or prod servers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "Without a strong testing culture, effort placed into software lifecycle automation will end in catastrophe. The following is a brief overview of testing best practices.\n",
    "\n",
    "## Types of Tests\n",
    "* **Unit:** A test of an individual unit of code, in isolation from other units.\n",
    "* **Integration:** A test of multiple units interoperating with one another.\n",
    "* **System:** Validation of an entire, integrated system against requirements.\n",
    "* **Benchmarks:** A test that focuses on the performance characteristics of a unit or system over its correctness.\n",
    "\n",
    "## Best Practices for Software Testing\n",
    "Tests can be difficult and expensive to write and maintain, and a brittle or misleading test can be worse than no test at all. Below are a few guidelines that will help you write tests that maximize value while minimizing test volume. Many of these guidelines have been adapted from [Working Effectively With Unit Tests](https://leanpub.com/wewut/read) by Jay Fields.\n",
    "\n",
    "### Write Tests for Readability\n",
    "Tests are read more often than they are written, and thus need to be written in a way that reflects this reality. A test will be read when it fails, when the running application fails and the test didn't, or when a developer wants to add a backwards-compatible change to the feature a test validates. In all of these situations, the test communicates an assumption about the code's behavior that needs to be validated.\n",
    "\n",
    "#### DAMP, not DRY\n",
    "In the same way that **D**on't **R**epeat **Y**ourself (DRY) principles improve the orthogonality of a codebase, **D**escriptive **A**nd **M**eaningful **P**hrases (DAMP) principles can improve its readability. When writing tests, don't worry about de-duplicating code and procedures, instead focus on placing everything a reader needs to understand your test into the test itself. By DRYing out your test and breaking fixtures out into reusable components that are not colocated, you've increased the cognitive effort needed to derive value from your test. A great discussion of this can be found in this [Stack Overflow answer](https://stackoverflow.com/a/11837973).\n",
    "\n",
    "#### Expect Literals\n",
    "When possible, an assertion should compare output to literal values, and not objects or variables containing values. Helper functions that generate values to compare should also be avoided in favor of a literal value written straight into the test. Doing so makes it significantly easier for a reader of your test to understand what the test is expecting, and examine how the behavior of the code may have diverged from that expectation, without having to search through and understand ancillary logic.\n",
    "\n",
    "#### One Assertion Per test\n",
    "Each test should be written, when possible, to include a single assertion at the very end. This practice has a positive impact on readability, and prevents the first failed assertion in a test from halting execution, obscuring the success or failure of subsequent assertions. A single assertion per test also tends to improve the durability of your tests, as it divides your validation into behavioral facets of a unit of code. A small change to some facet no longer necessitates that all tests surrounding that unit need to be rewritten.\n",
    "\n",
    "### Maximize Test Value\n",
    "For codebases of any non-trivial size it is rarely viable to achieve 100% coverage, so one must be deliberate about which tests they write, and more importantly, which tests are maintained long-term. The following heuristics can help with this selection process.\n",
    "\n",
    "#### Write Tests for Risk Factors\n",
    "When prioiritizing test coverage at the application-level, focus first primary drivers of risk _(i.e., numerical errors or double-charging in an ecommerce application)_ and critical use cases _(i.e., adding to one's shopping cart)_. When prioritizing coverage on a class- or component-level, prioritize public interfaces.\n",
    "\n",
    "#### Eliminate Low-Value Tests\n",
    "If a test costs more time to maintain than the time it saves in discovering defects and validating changes, delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
