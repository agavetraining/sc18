{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Setting Stuff Up</h2>\n",
    "\n",
    "Here we import some packages that we'll need in various places. We'll also load all the variables we set in config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/agave\n",
      "Requirement already up-to-date: setvar in /opt/conda/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "AGAVE_APP_DEPLOYMENT_PATH=agave-deployment\n",
      "AGAVE_APP_NAME=funwave-tvd-jupyter-dooley\n",
      "AGAVE_EXECUTION_SYSTEM_ID=jupyter-exec-dooley\n",
      "AGAVE_EXECUTION_SYSTEM_SCRATCH_DIR=/home/jovyan\n",
      "AGAVE_EXECUTION_SYSTEM_WORK_DIR=/home/jovyan\n",
      "AGAVE_JSON_PARSER=jq\n",
      "AGAVE_STORAGE_HOME_DIR=/home/jovyan\n",
      "AGAVE_STORAGE_SYSTEM_ID=jupyter-storage-dooley\n",
      "AGAVE_SYSTEM_HOME_DIR=/home/jovyan\n",
      "AGAVE_SYSTEM_PORT=10022\n",
      "AGAVE_SYSTEM_SITE_DOMAIN=agaveplatform.org\n",
      "AGAVE_TENANTS_API_BASEURL=https://public.agaveapi.co/tenants\n",
      "AGAVE_USERNAME=dooley\n",
      "APP_NAME=funwave-tvd-jupyter-dooley\n",
      "DEPLOYMENT_PATH=agave-deployment\n",
      "DOCKERHUB_NAME=dooley\n",
      "DOMAIN=agaveplatform.org\n",
      "EMAIL=foo@example.com\n",
      "EXEC_MACHINE=jupyter-exec-dooley\n",
      "HOME_DIR=/home/jovyan\n",
      "MACHINE_IP=52.15.62.13\n",
      "MACHINE_NAME=jupyter\n",
      "MACHINE_USERNAME=jovyan\n",
      "PBTOK=o.JJ4g3jgQEq0J5GM1FI5eRDwVIUwxFAWe\n",
      "PORT=10022\n",
      "REQUESTBIN_URL=https://requestbin.agaveapi.co/16x9z6w1\n",
      "SCRATCH_DIR=/home/jovyan\n",
      "STORAGE_MACHINE=jupyter-storage-dooley\n",
      "VM_IPADDRESS=52.15.72.79\n",
      "WORK_DIR=/home/jovyan\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ~/agave\n",
    "\n",
    "%cd ~/agave\n",
    "\n",
    "!pip3 install --upgrade setvar\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from setvar import *\n",
    "from time import sleep\n",
    "\n",
    "# This cell enables inline plotting in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "loadvar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running locally using Docker Compose, you will need to pull the ip and port of your reverse tunnel from the sandbox. Uncomment the following command, and enter below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOLLOWING ALONG AT HOME  \n",
    "\n",
    "If you are following along at home using the docker-compose stack, you will need to run the following cell to get the hsotname and port of your tcp tunnel so Agave can contact your system without a public IP address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.tcp.ngrok.io:12873\n",
      "Reading file `ngrok_port.txt'\n",
      "Reading file `ngrok_host.txt'\n",
      "AGAVE_SYSTEM_HOST=0.tcp.ngrok.io\n",
      "AGAVE_SYSTEM_PORT=12873\n",
      "VM_IPADDRESS=52.15.62.13\n"
     ]
    }
   ],
   "source": [
    "if os.environ.get('USE_TUNNEL') == 'True': \n",
    "    # fetch the hostname and port of the reverse tunnel running in the sandbox \n",
    "    # so Agave can connect to our local sandbox\n",
    "    !echo $(ssh -q -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null sandbox 'curl -s  http://localhost:4040/api/tunnels | jq -r '.tunnels[0].public_url'') > ngrok_url.txt  \n",
    "    !cat ngrok_url.txt | sed 's|^tcp://||'\n",
    "    !cat ngrok_url.txt | sed 's|^tcp://||' | sed -r 's#(.*):(.*)#\\1#' > ngrok_host.txt\n",
    "    !cat ngrok_url.txt | sed 's|^tcp://||' | sed -r 's#(.*):(.*)#\\2#' > ngrok_port.txt\n",
    "\n",
    "    # set the environment variables otherwise set when running in a training cluster\n",
    "    os.environ['VM_PORT'] = readfile('ngrok_port.txt').strip()\n",
    "    os.environ['VM_MACHINE'] = readfile('ngrok_host.txt').strip()\n",
    "    setvar(\"\"\"\n",
    "    AGAVE_SYSTEM_HOST=$VM_MACHINE\n",
    "    AGAVE_SYSTEM_PORT=$VM_PORT\n",
    "    VM_IPADDRESS=$(getent hosts ${VM_MACHINE}|cut -d' ' -f1)\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Storage Machine   \n",
    "\n",
    "Agave wants to know which place (or places) you want to store the data associated with your jobs. Here, we're going to set that up. Authentication to the storage machine will be through SSH keys. The key and public key files, however, contain newlines. To encode them in Json (the data format used by Agave), we will run the jsonpki command on each file. Next, we will store its contents in the environment for use by setvar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jsonpki --public ~/.ssh/id_rsa.pub > ./id_rsa.pub.txt\n",
    "!jsonpki --private ~/.ssh/id_rsa > ./id_rsa.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file `id_rsa.pub.txt'\n",
      "Reading file `id_rsa.txt'\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PUB_KEY\"]=readfile(\"id_rsa.pub.txt\").strip()\n",
    "os.environ[\"PRIV_KEY\"]=readfile(\"id_rsa.txt\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next cell, we create the json file used to describe the storage machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing file `jupyter-storage-dooley.txt'\n"
     ]
    }
   ],
   "source": [
    "writefile(\"${AGAVE_STORAGE_SYSTEM_ID}.txt\",\"\"\"{\n",
    "    \"id\": \"${AGAVE_STORAGE_SYSTEM_ID}\",\n",
    "    \"name\": \"${MACHINE_NAME} storage (${MACHINE_USERNAME})\",\n",
    "    \"description\": \"The ${MACHINE_NAME} computer\",\n",
    "    \"site\": \"${AGAVE_SYSTEM_SITE_DOMAIN}\",\n",
    "    \"type\": \"STORAGE\",\n",
    "    \"storage\": {\n",
    "        \"host\": \"${AGAVE_SYSTEM_HOST}\",\n",
    "        \"port\": ${AGAVE_SYSTEM_PORT},\n",
    "        \"protocol\": \"SFTP\",\n",
    "        \"rootDir\": \"/\",\n",
    "        \"homeDir\": \"${AGAVE_STORAGE_HOME_DIR}\",\n",
    "        \"auth\": {\n",
    "          \"username\" : \"${MACHINE_USERNAME}\",\n",
    "          \"publicKey\" : \"${PUB_KEY}\",\n",
    "          \"privateKey\" : \"${PRIV_KEY}\",\n",
    "          \"type\" : \"SSHKEYS\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we tell Agave about the machine. You can re-run the previous cell and the next one if you want to change the definition of your storage machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0m\u001b[1;0mSuccessfully added system jupyter-storage-dooley\u001b[0m\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!systems-addupdate -F ${AGAVE_STORAGE_SYSTEM_ID}.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we run the Agave command `files-list`. This provides a check that we've set up the storage machine correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan\r\n",
      "/home/jovyan/.bash_logout\r\n",
      "/home/jovyan/.bashrc\r\n",
      "/home/jovyan/.cache\r\n",
      "/home/jovyan/.ngrok2\r\n"
     ]
    }
   ],
   "source": [
    "!files-list -S ${AGAVE_STORAGE_SYSTEM_ID} ./ | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Setting up the Execution Machine</h2>\n",
    "\n",
    "You may not always wish to store your data on the same machine you run your jobs on. However, in this tutorial, we will assume that you do. The description for the execution machine is much like the storage machine. However, there are a few more pieces of information you'll need to provide. In this example, we are going to call commands directly on the host as opposed to using a batch queue scheduler. It is slightly simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing file `jupyter-exec-dooley.txt'\n"
     ]
    }
   ],
   "source": [
    "# Edit any parts of this file that you know need to be changed for your machine.\n",
    "writefile(\"${AGAVE_EXECUTION_SYSTEM_ID}.txt\",\"\"\"\n",
    "{\n",
    "    \"id\": \"${AGAVE_EXECUTION_SYSTEM_ID}\",\n",
    "    \"name\": \"${MACHINE_NAME} (${MACHINE_USERNAME})\",\n",
    "    \"description\": \"The ${MACHINE_NAME} computer\",\n",
    "    \"site\": \"${AGAVE_SYSTEM_SITE_DOMAIN}\",\n",
    "    \"public\": false,\n",
    "    \"status\": \"UP\",\n",
    "    \"type\": \"EXECUTION\",\n",
    "    \"executionType\": \"CLI\",\n",
    "    \"scheduler\" : \"FORK\",\n",
    "    \"environment\": null,\n",
    "    \"scratchDir\" : \"${AGAVE_EXECUTION_SYSTEM_SCRATCH_DIR}\",\n",
    "    \"workDir\": \"${AGAVE_EXECUTION_SYSTEM_WORK_DIR}\",\n",
    "    \"queues\": [\n",
    "        {\n",
    "            \"name\": \"none\",\n",
    "            \"default\": true,\n",
    "            \"maxJobs\": 10,\n",
    "            \"maxUserJobs\": 10,\n",
    "            \"maxNodes\": 6,\n",
    "            \"maxProcessorsPerNode\": 6,\n",
    "            \"minProcessorsPerNode\": 1,\n",
    "            \"maxRequestedTime\": \"00:30:00\"\n",
    "        }\n",
    "    ],\n",
    "    \"login\": {\n",
    "        \"auth\": {\n",
    "          \"username\" : \"${MACHINE_USERNAME}\",\n",
    "          \"publicKey\" : \"${PUB_KEY}\",\n",
    "          \"privateKey\" : \"${PRIV_KEY}\",\n",
    "          \"type\" : \"SSHKEYS\"\n",
    "        },\n",
    "        \"host\": \"${AGAVE_SYSTEM_HOST}\",\n",
    "        \"port\": ${AGAVE_SYSTEM_PORT},\n",
    "        \"protocol\": \"SSH\"\n",
    "    },\n",
    "    \"maxSystemJobs\": 50,\n",
    "    \"maxSystemJobsPerUser\": 50,\n",
    "    \"storage\": {\n",
    "        \"host\": \"${AGAVE_SYSTEM_HOST}\",\n",
    "        \"port\": ${AGAVE_SYSTEM_PORT},\n",
    "        \"protocol\": \"SFTP\",\n",
    "        \"rootDir\": \"/\",\n",
    "        \"homeDir\": \"${AGAVE_SYSTEM_HOME_DIR}\",\n",
    "        \"auth\": {\n",
    "          \"username\" : \"${MACHINE_USERNAME}\",\n",
    "          \"publicKey\" : \"${PUB_KEY}\",\n",
    "          \"privateKey\" : \"${PRIV_KEY}\",\n",
    "          \"type\" : \"SSHKEYS\"\n",
    "        }\n",
    "    }\n",
    "}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0m\u001b[1;0mSuccessfully added system jupyter-exec-dooley\u001b[0m\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!systems-addupdate -F ${AGAVE_EXECUTION_SYSTEM_ID}.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan\r\n",
      "/home/jovyan/.bash_logout\r\n",
      "/home/jovyan/.bashrc\r\n",
      "/home/jovyan/.cache\r\n",
      "/home/jovyan/.ngrok2\r\n"
     ]
    }
   ],
   "source": [
    "# Test to see if this worked...\n",
    "!files-list -S ${AGAVE_EXECUTION_SYSTEM_ID} ./ | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create the Application</h3>\n",
    "Agave allows us to describe custom allocations, limiting users to run a specific job. In this case, we're going to create a simple \"fork\" scheduler that just takes the command we want to run as a job parameter. The wrapper file is a shell script we will run on the execution machine. If we were using a scheduler, this would be our batch file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing file `fork-wrapper.txt'\n"
     ]
    }
   ],
   "source": [
    "writefile(\"fork-wrapper.txt\",\"\"\"\n",
    "#!/bin/bash\n",
    "\\${command}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Agave commands, we make a directory on the storage server an deploy our wrapper file there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0mSuccessfully created folder agave-deployment\u001b[0m\n",
      "Uploading fork-wrapper.txt...\n",
      "######################################################################## 100.0%\n"
     ]
    }
   ],
   "source": [
    "!files-mkdir -S ${AGAVE_STORAGE_SYSTEM_ID} -N ${AGAVE_APP_DEPLOYMENT_PATH}\n",
    "!files-upload -F fork-wrapper.txt -S ${AGAVE_STORAGE_SYSTEM_ID} ${AGAVE_APP_DEPLOYMENT_PATH}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All agave applications require a test file. The test file is a free form text file which allows you to specify what resources you might need to test your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing file `fork-test.txt'\n"
     ]
    }
   ],
   "source": [
    "writefile(\"fork-test.txt\",\"\"\"\n",
    "command=date\n",
    "fork-wrapper.txt\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0mSuccessfully created folder agave-deployment\u001b[0m\n",
      "Uploading fork-test.txt...\n",
      "######################################################################## 100.0%\n"
     ]
    }
   ],
   "source": [
    "!files-mkdir -S ${AGAVE_STORAGE_SYSTEM_ID} -N ${AGAVE_APP_DEPLOYMENT_PATH}\n",
    "!files-upload -F fork-test.txt -S ${AGAVE_STORAGE_SYSTEM_ID} ${AGAVE_APP_DEPLOYMENT_PATH}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like everything else in Agave, we describe our application with JSON. We specifiy which machines the application will use, what method it will use for submitting jobs, job parameters and files, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing file `fork-app.txt'\n"
     ]
    }
   ],
   "source": [
    "writefile(\"fork-app.txt\",\"\"\"\n",
    "{  \n",
    "   \"name\":\"${AGAVE_USERNAME}-${MACHINE_NAME}-fork\",\n",
    "   \"version\":\"1.0\",\n",
    "   \"label\":\"Runs a command\",\n",
    "   \"shortDescription\":\"Runs a command\",\n",
    "   \"longDescription\":\"\",\n",
    "   \"deploymentSystem\":\"${AGAVE_STORAGE_SYSTEM_ID}\",\n",
    "   \"deploymentPath\":\"${AGAVE_APP_DEPLOYMENT_PATH}\",\n",
    "   \"templatePath\":\"fork-wrapper.txt\",\n",
    "   \"testPath\":\"fork-test.txt\",\n",
    "   \"executionSystem\":\"${AGAVE_EXECUTION_SYSTEM_ID}\",\n",
    "   \"executionType\":\"CLI\",\n",
    "   \"parallelism\":\"SERIAL\",\n",
    "   \"modules\":[],\n",
    "   \"inputs\":[\n",
    "         {   \n",
    "         \"id\":\"datafile\",\n",
    "         \"details\":{  \n",
    "            \"label\":\"Data file\",\n",
    "            \"description\":\"\",\n",
    "            \"argument\":null,\n",
    "            \"showArgument\":false\n",
    "         },\n",
    "         \"value\":{  \n",
    "            \"default\":\"/dev/null\",\n",
    "            \"order\":0,\n",
    "            \"required\":false,\n",
    "            \"validator\":\"\",\n",
    "            \"visible\":true\n",
    "         }\n",
    "      }   \n",
    "   ],\n",
    "   \"parameters\":[{\n",
    "     \"id\" : \"command\",\n",
    "     \"value\" : {\n",
    "       \"visible\":true,\n",
    "       \"required\":true,\n",
    "       \"type\":\"string\",\n",
    "       \"order\":0,\n",
    "       \"enquote\":false,\n",
    "       \"default\":\"/bin/date\",\n",
    "       \"validator\":null\n",
    "     },\n",
    "     \"details\":{\n",
    "         \"label\": \"Command to run\",\n",
    "         \"description\": \"This is the actual command you want to run. ex. df -h -d 1\",\n",
    "         \"argument\": null,\n",
    "         \"showArgument\": false,\n",
    "         \"repeatArgument\": false\n",
    "     },\n",
    "     \"semantics\":{\n",
    "         \"label\": \"Command to run\",\n",
    "         \"description\": \"This is the actual command you want to run. ex. df -h -d 1\",\n",
    "         \"argument\": null,\n",
    "         \"showArgument\": false,\n",
    "         \"repeatArgument\": false\n",
    "     }\n",
    "   }],\n",
    "   \"outputs\":[]\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0m\u001b[1;0mSuccessfully added app dooley-jupyter-fork-1.0\u001b[0m\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!apps-addupdate -F fork-app.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Running Jobs</h2>\n",
    "Now that we have specified our application using Agave, it is time to try running jobs. To start a job we, once again, create a Json file. The Json file describes the app, what resource to run on, as well as how and when to send notifications. Notifications are delivered by callback url. EMAIL is the easiest type to configure, but we show here how to send webhook notifications to the popular [RequestBin](https://requestb.in/). \n",
    "\n",
    "Before we configure our notification, we need to create a requestbin to use. There are convenience commands to interact with requestbin built into the Agave CLI. We will use those to get our URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rburl = !requestbin-create \n",
    "os.environ['REQUESTBIN_URL'] = rburl[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a URL to recieve webhooks from our job, Let's look at our job request. The way this job is configured, it will send the requestbin notifications for every job event until the job reaches a terminal state. For a full list of job events, please see http://docs.agaveplatform.org/#job-monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing file `job.txt'\n"
     ]
    }
   ],
   "source": [
    "writefile(\"job.txt\",\"\"\"\n",
    " {\n",
    "   \"name\":\"fork-command-1\",\n",
    "   \"appId\": \"${AGAVE_USERNAME}-${MACHINE_NAME}-fork-1.0\",\n",
    "   \"executionSystem\": \"${AGAVE_EXECUTION_SYSTEM_ID}\",\n",
    "   \"archive\": false,\n",
    "   \"notifications\": [\n",
    "    {\n",
    "      \"url\":\"${REQUESTBIN_URL}?event=\\${EVENT}&jobid=\\${JOB_ID}\",\n",
    "      \"event\":\"*\",\n",
    "      \"persistent\":\"true\"\n",
    "    }\n",
    "   ],\n",
    "   \"parameters\": {\n",
    "     \"command\":\"echo hello\"\n",
    "   }\n",
    " }\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the setvar() command can evalute `$()` style bash shell substitutions, we will use it to submit our job. This will capture the output of the submit command, and allow us to parse it for the JOB_ID. We'll use the JOB_ID in several subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT=Successfully submitted job 1880859020890083816-242ac11b-0001-007\n",
      "JOB_ID=1880859020890083816-242ac11b-0001-007\n"
     ]
    }
   ],
   "source": [
    "setvar(\"\"\"\n",
    "# Capture the output of the job submit command\n",
    "OUTPUT=$(jobs-submit -F job.txt)\n",
    "# Parse out the job id from the output\n",
    "JOB_ID=$(echo $OUTPUT | cut -d' ' -f4)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Job Monitoring and Output</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the job is running, the requestbin you registered will receive webhooks from Agave every time a job event occurs. To monitor this in real time, evaluate the next cell an visit the printed url in your browser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://requestbin.agaveapi.co/16x9z6w1?inspect\n"
     ]
    }
   ],
   "source": [
    "print ('%s?inspect'%os.environ['REQUESTBIN_URL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can also monitor the job status by polling. Note that the notifications you receive via email and webhook are less wasteful of resources. However, we show you this for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAT=PENDING\n",
      "STAT=PENDING\n",
      "STAT=PENDING\n",
      "STAT=PENDING\n",
      "STAT=STAGED\n",
      "STAT=STAGED\n",
      "STAT=SUBMITTING\n",
      "STAT=SUBMITTING\n",
      "STAT=SUBMITTING\n",
      "STAT=SUBMITTING\n",
      "STAT=SUBMITTING\n",
      "STAT=RUNNING\n",
      "STAT=FINISHED\n"
     ]
    }
   ],
   "source": [
    "for iter in range(20):\n",
    "    setvar(\"STAT=$(jobs-status $JOB_ID)\")\n",
    "    stat = os.environ[\"STAT\"]\n",
    "    sleep(5.0)\n",
    "    if stat == \"FINISHED\" or stat == \"FAILED\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The jobs-history command provides you a record of the steps of what your job did. If your job fails for some reason, this is your best diagnostic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0mJob accepted and queued for submission.\r\n",
      "Skipping staging. No input data associated with this job.\r\n",
      "Preparing job for submission.\r\n",
      "Attempt 1 to submit job\r\n",
      "Fetching app assets from agave://jupyter-storage-dooley/agave-deployment\r\n",
      "Staging runtime assets to agave://jupyter-exec-dooley//home/jovyan/dooley/job-1880859020890083816-242ac11b-0001-007-fork-command-1\r\n",
      "CLI job successfully forked as process id 536\r\n",
      "CLI job successfully forked as process id 536\r\n",
      "Job receieved duplicate RUNNING notification\r\n",
      "Job completed execution\r\n",
      "Job completed. Skipping archiving at user request.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!jobs-history ${JOB_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command shows you the job id's and status of the last 5 jobs you ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0m1880859020890083816-242ac11b-0001-007 FINISHED\r\n",
      "1797975497817592296-242ac11b-0001-007 FAILED\r\n",
      "701738006829723160-242ac11b-0001-007 FINISHED\r\n",
      "4218907132382604825-242ac11c-0001-007 FINISHED\r\n",
      "8855889690296193511-242ac11b-0001-007 FINISHED\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!jobs-list -l 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next command provides you with a list of all the files generated by your job. You can use it to figure out which files you want to retrieve with jobs-output-get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0m| type | length | name                  |\r\n",
      "| ---- | ------ | ----                  |\r\n",
      "| file | 68     | .agave.archive        |\r\n",
      "| file | 399    | .agave.log            |\r\n",
      "| file | 0      | fork-command-1.err    |\r\n",
      "| file | 2481   | fork-command-1.ipcexe |\r\n",
      "| file | 6      | fork-command-1.out    |\r\n",
      "| file | 4      | fork-command-1.pid    |\r\n",
      "| file | 29     | fork-test.txt         |\r\n",
      "| file | 22     | fork-wrapper.txt      |\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!jobs-output-list --rich --filter=type,length,name ${JOB_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the standard output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################## 100.0%\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "!jobs-output-get ${JOB_ID} fork-command-1.out\n",
    "!cat fork-command-1.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the standard error output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    }
   ],
   "source": [
    "!jobs-output-get ${JOB_ID} fork-command-1.err\n",
    "!cat fork-command-1.err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Automating</h3>\n",
    "Because we're working in Python, we can simply glue the above steps together and create a script to run jobs for us and fetch the standard output. Let's do that next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing runagavecmd.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile runagavecmd.py\n",
    "from setvar import *\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "def runagavecmd(cmd,infile=None):\n",
    "    setvar(\"REMOTE_COMMAND=\"+cmd)\n",
    "    setvar(\"REQUESTBIN_URL=$(requestbin-create)\")\n",
    "    print(\"\")\n",
    "    print(\" ** QUERY STRING FOR REQUESTBIN **\")\n",
    "    print('%s?inspect'%os.environ['REQUESTBIN_URL'])\n",
    "    print(\"\")\n",
    "    # The input file is an optional parameter, both\n",
    "    # to our function and to the Agave application.\n",
    "    if infile == None:\n",
    "        setvar(\"INPUTS={}\")\n",
    "    else:\n",
    "        setvar('INPUTS={\"datafile\":\"'+infile+'\"}')\n",
    "    setvar(\"JOB_FILE=job-remote-$PID.txt\")\n",
    "    # Create the Json for the job file.\n",
    "    writefile(\"$JOB_FILE\",\"\"\"\n",
    " {\n",
    "   \"name\":\"fork-command-1\",\n",
    "   \"appId\": \"${AGAVE_USERNAME}-${MACHINE_NAME}-fork-1.0\",\n",
    "   \"executionSystem\": \"${AGAVE_EXECUTION_SYSTEM_ID}\",\n",
    "   \"archive\": false,\n",
    "   \"notifications\": [\n",
    "    {\n",
    "      \"url\":\"${REQUESTBIN_URL}?event=\\${EVENT}&jobid=\\${JOB_ID}\",\n",
    "      \"event\":\"*\",\n",
    "      \"persistent\":\"true\"\n",
    "    }\n",
    "   ],\n",
    "   \"parameters\": {\n",
    "     \"command\":\"${REMOTE_COMMAND}\"\n",
    "   },\n",
    "   \"inputs\":${INPUTS}\n",
    " }\"\"\")\n",
    "    # Run the job and capture the output.\n",
    "    setvar(\"\"\"\n",
    "# Capture the output of the job submit command\n",
    "OUTPUT=$(jobs-submit -F $JOB_FILE)\n",
    "# Parse out the job id from the output\n",
    "JOB_ID=$(echo $OUTPUT | cut -d' ' -f4)\n",
    "\"\"\")\n",
    "    # Poll and wait for the job to finish.\n",
    "    for iter in range(80): # Excessively generous\n",
    "        setvar(\"STAT=$(jobs-status $JOB_ID)\")\n",
    "        stat = os.environ[\"STAT\"]\n",
    "        sleep(5.0)\n",
    "        if stat == \"FINISHED\" or stat == \"FAILED\":\n",
    "            break\n",
    "    # Fetch the job output from the remote machine\n",
    "    setvar(\"CMD=jobs-output-get ${JOB_ID} fork-command-1.out\")\n",
    "    os.system(os.environ[\"CMD\"])\n",
    "    print(\"All done! Output follows.\")\n",
    "    # Load the output into memory\n",
    "    output=readfile(\"fork-command-1.out\")\n",
    "    print(\"=\" * 70)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'runagavecmd' from '/home/jovyan/agave/runagavecmd.py'>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import runagavecmd as r\n",
    "import imp\n",
    "imp.reload(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOTE_COMMAND=lscpu\n",
      "REQUESTBIN_URL=https://requestbin.agaveapi.co/1ib4lq11\n",
      "\n",
      " ** QUERY STRING FOR REQUESTBIN **\n",
      "https://requestbin.agaveapi.co/1ib4lq11?inspect\n",
      "\n",
      "INPUTS={}\n",
      "JOB_FILE=job-remote-2816.txt\n",
      "Writing file `job-remote-2816.txt'\n",
      "OUTPUT=Successfully submitted job 7232517120724963816-242ac11b-0001-007\n",
      "JOB_ID=7232517120724963816-242ac11b-0001-007\n",
      "STAT=PENDING\n",
      "STAT=PENDING\n",
      "STAT=PENDING\n",
      "STAT=PENDING\n",
      "STAT=STAGED\n",
      "STAT=STAGED\n",
      "STAT=SUBMITTING\n",
      "STAT=SUBMITTING\n",
      "STAT=SUBMITTING\n",
      "STAT=SUBMITTING\n",
      "STAT=SUBMITTING\n",
      "STAT=FINISHED\n",
      "CMD=jobs-output-get 7232517120724963816-242ac11b-0001-007 fork-command-1.out\n",
      "All done! Output follows.\n",
      "Reading file `fork-command-1.out'\n",
      "======================================================================\n",
      "Architecture:          x86_64\n",
      "CPU op-mode(s):        32-bit, 64-bit\n",
      "Byte Order:            Little Endian\n",
      "CPU(s):                2\n",
      "On-line CPU(s) list:   0,1\n",
      "Thread(s) per core:    1\n",
      "Core(s) per socket:    2\n",
      "Socket(s):             1\n",
      "Vendor ID:             GenuineIntel\n",
      "CPU family:            6\n",
      "Model:                 158\n",
      "Model name:            Intel(R) Core(TM) i7-7820HQ CPU @ 2.90GHz\n",
      "Stepping:              9\n",
      "CPU MHz:               2904.000\n",
      "BogoMIPS:              5808.00\n",
      "Hypervisor vendor:     KVM\n",
      "Virtualization type:   full\n",
      "L1d cache:             32K\n",
      "L1i cache:             32K\n",
      "L2 cache:              256K\n",
      "L3 cache:              8192K\n",
      "Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc pni pclmulqdq ssse3 cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx rdrand hypervisor lahf_lm abm 3dnowprefetch fsgsbase avx2 invpcid rdseed clflushopt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r.runagavecmd(\"lscpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Permissions and Sharing</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the users and the permssions they have to look at the given job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0mdooley READ WRITE \r\n",
      "training002 READ \u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!jobs-pems-list ${JOB_ID}\n",
    "#!jobs-search --filter=id,owner --rich -l 1000 owner.neq=${AGAVE_USERNAME} status=FINISHED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0mSuccessfully updated permission for training002\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# now pair off with your neighbor and both of you share your job with them.\n",
    "# For now, just give read access\n",
    "!jobs-pems-update -u training002 -p READ ${JOB_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8337428240199773720-242ac11b-0001-007\n"
     ]
    }
   ],
   "source": [
    "# Now let's see if we can see our neighbor's job\n",
    "# Now let's see if we can see our neighbor's job\n",
    "shared_job = !jobs-search --filter=id -l 1000 owner.neq=${AGAVE_USERNAME} status=FINISHED\n",
    "os.environ['SHARED_JOB_ID'] = shared_job[0]\n",
    "os.environ['SHARED_JOB_ID'] = '8337428240199773720-242ac11b-0001-007'\n",
    "print(os.environ['SHARED_JOB_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permissions are just that, permitting someone to do something. You said your neighbor could view your job. Let's see what that means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0m8337428240199773720-242ac11b-0001-007 FINISHED\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# You already searched for the job and found it, so you should be able to lis\n",
    "# an view the details\n",
    "!jobs-list $SHARED_JOB_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0mJob completed. Skipping archiving at user request.\r\n",
      "Job completed execution\r\n",
      "Job started running\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# You should also be able to view the history. Here we'll just return the last few \n",
    "# events. Notice the history event showed up history event\n",
    "!jobs-history --limit 3 --order desc $SHARED_JOB_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mFailed to authenticate to shelob.hpc.lsu.edu\u001b[0m\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# You can also view their job output\n",
    "!jobs-output-list -L $SHARED_JOB_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mNo job found with job id 8337428240199773720-242ac11b-0001-007\u001b[0m\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# What if we no longer want to see the job. Let's delete it.\n",
    "!jobs-delete $SHARED_JOB_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doah! We can't delete the shared job because we weren't granted write permission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0mSuccessfully updated permission for training002\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Let's grant write access and see what we can do\n",
    "!jobs-pems-update -u training002 -p READ_WRITE ${JOB_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mNo job found with job id 8337428240199773720-242ac11b-0001-007\u001b[0m\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Now let's see if we can delete the shared job\n",
    "!jobs-delete $SHARED_JOB_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0mSuccessfully restored job 8337428240199773720-242ac11b-0001-007.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Wait, now we don't have anything to work with. \n",
    "# No worries. Agave doens't really delete anything. Your job is still there\n",
    "# We just need to restore it.\n",
    "!jobs-restore $SHARED_JOB_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mUnable to locate app templatePath at agave-deployment/fork-wrapper.txt on system nectar-storage-training005. App is not available for execution and will be disabled.\u001b[0m\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Now let's try to rerun the job\n",
    "!jobs-resubmit $SHARED_JOB_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nectar-exec-training005', 'training005-training005-fork-1.0']\n"
     ]
    }
   ],
   "source": [
    "# Well, what app did they use in the job?\n",
    "\n",
    "shared_job = !jobs-list -v --filter=executionSystem,appId $SHARED_JOB_ID | jq -r '. | [ .executionSystem , .appId] | .[]'\n",
    "print(shared_job)\n",
    "os.environ['SHARED_JOB_APP'] = shared_job[1]\n",
    "os.environ['SHARED_JOB_SYSTEM'] = shared_job[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0mtraining005 READ WRITE EXECUTE \r\n",
      "stevenrbrandt READ WRITE EXECUTE \u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Hmm, do we have access to the app?\n",
    "! apps-pems-list $SHARED_JOB_APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0m\u001b[1;0mSuccessfully updated permission for training002\u001b[0m\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Oh, we don't have permission to even view the app. Guess our job permissions\n",
    "# don't extend to the application. Let's be a good neighbor and share our apps\n",
    "# with each other\n",
    "! apps-pems-update -u training002 -p READ \"${AGAVE_USERNAME}-${MACHINE_NAME}-fork-1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0mtraining005 READ WRITE EXECUTE \r\n",
      "stevenrbrandt READ WRITE EXECUTE \u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Now do we have access to the app?\n",
    "! apps-pems-list $SHARED_JOB_APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0m\u001b[1;0mSuccessfully updated permission for training002\u001b[0m\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Score, but wait, do I need execute to run? We should granb that too.\n",
    "# Hmm, do we have access to the app?\n",
    "! apps-pems-update -u training002 -p EXECUTE \"${AGAVE_USERNAME}-${MACHINE_NAME}-fork-1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0mtraining005 READ WRITE EXECUTE \r\n",
      "stevenrbrandt READ WRITE EXECUTE \u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Now do we have access to the app?\n",
    "! apps-pems-list $SHARED_JOB_APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0m\u001b[1;0mSuccessfully updated permission for training002\u001b[0m\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# I guess permissions aren't hierachical. Now i can execute it (I think), but I can't\n",
    "# read it. How aabout we grant read_execute instead\n",
    "! apps-pems-update -u training002 -p READ_EXECUTE \"${AGAVE_USERNAME}-${MACHINE_NAME}-fork-1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0mtraining005 READ WRITE EXECUTE \r\n",
      "stevenrbrandt READ WRITE EXECUTE \u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Now do we have access to the app?\n",
    "! apps-pems-list $SHARED_JOB_APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mApp is not available for execution\u001b[0m\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# So now we can rerun our neighbor's job, right\n",
    "!jobs-resubmit -v $SHARED_JOB_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0mtraining005 OWNER\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# drat. why can't we run now? Do we have system access?\n",
    "!systems-roles-list $SHARED_JOB_SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0mSuccessfully updated roles for user training002 on nectar-execstevenrbrandt\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# ok, let's skip to the end. we'll just realize we should grant a user rather than guest role\n",
    "# to the system\n",
    "!systems-roles-addupdate -u training002 -r USER $AGAVE_EXECUTION_SYSTEM_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0mtraining005 OWNER\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# that should work, right?\n",
    "!systems-roles-list $SHARED_JOB_SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So can we run the job now?\n",
    "resubmitted_job_id = ! jobs-resubmit -v --filter=id $SHARED_JOB_ID | jq -r '.id'\n",
    "os.environ['RESUBMITTED_JOB_ID'] = resubmitted_job_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "App is not available for execution\n",
      "\u001b[1;31mNo job found with job id App\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# yay. wait, who owns the data?\n",
    "print (resubmitted_job_id[0])\n",
    "! jobs-pems-list $RESUBMITTED_JOB_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mNo job found with job id App\u001b[0m\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# mine, mine, mine, mine, mine, mine, mine, mine, mine, mine, mine, mine, mine\n",
    "# kill it, we're moving on.\n",
    "! jobs-stop $RESUBMITTED_JOB_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we can also share data a few ways\n",
    "job_output_url = ! jobs-output-list -v --filter=_links $JOB_ID fork-command-1.out | jq -r '.[0]._links.self.href'\n",
    "os.environ['JOB_OUTPUT_URL'] = job_output_url[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "postit_url = ! postits-create -m 3 -l 86400 -V $JOB_OUTPUT_URL | jq -r '.result._links.self.href' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://public.agaveapi.co/postits/v2/540f6a184d7c0c280587218958e726b8\n"
     ]
    }
   ],
   "source": [
    "# click on the link a few times to see it working.\n",
    "print (postit_url[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0mSuccessfully updated permission for training002\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# you can also share your data via the files api\n",
    "# let's share the job directory with each other\n",
    "job_path = ! jobs-list -v $JOB_ID | jq -r '.outputPath'\n",
    "os.environ['JOB_OUTPUT_FOLDER_PATH'] = job_path[0]\n",
    "! files-pems-update -u training002 -p read -S $AGAVE_EXECUTION_SYSTEM_ID $JOB_OUTPUT_FOLDER_PATH/fork-command-1.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0mSuccessfully deleted job 3310103185399016985-242ac11b-0001-007\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!jobs-delete $JOB_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the TOGO web portal  \n",
    "\n",
    "Follow the link below to run your job from a web portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://togo.agaveplatform.org/app/#/apps/stevenrbrandt-sandbox-fork-1.0/run\r\n"
     ]
    }
   ],
   "source": [
    "!echo http://togo.agaveplatform.org/app/#/apps/${AGAVE_USERNAME}-${MACHINE_NAME}-fork-1.0/run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
