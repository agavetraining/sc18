{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/agave/funwave-tvd-jenkins-pipeline\n",
    "\n",
    "%cd ~/agave/funwave-tvd-jenkins-pipeline\n",
    "\n",
    "!pip3 install --upgrade setvar\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "from setvar import *\n",
    "\n",
    "# This cell enables inline plotting in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!auth-tokens-refresh\n",
    "setvar(\"\"\"\n",
    "AGAVE_STORAGE_SYSTEM_ID=sandbox-storage-${AGAVE_USERNAME}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating Your Build\n",
    "Automation is great, but things can quickly go wrong. This is why good testing practices are central to CI/CD. Having multiple, strong layers of testing guards in your automation pipeline allows you to assess the viability of each commit, determine whether or not code can be successfully integrated, and in advanced scenarios it allows for code to be automatically deployed to production without the direct oversight of a tightly controlled group of developers.\n",
    "\n",
    "There are many kinds of tests for validating different aspects of code viability _(discussed in greater detail in notebook 11)_. For our Funwave pipeline we will be focusing on performance as the primary assessment criteria, and thus we will be running a benchmark after each successful build.\n",
    "\n",
    "In this notebook, we'll be validating the performance of Funwave by running a strong scaling study. We've already collected a few points for a strong scaling study with processor counts of 1, 2, and 4. We're going to visualize our benchmark with Matplotlib, run another Benchmark with Jenkins to add additional data, and finally plot our benchmarking results including the lastest data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Initial Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've stored output files from FUNWAVE in /home/jovyan/notebooks/build/np_{1, 2, 4}. The output files are names DATE_COMMIT_RUN.out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/jovyan/notebooks/build/np_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting data\n",
    "The general flow for gathering and plotting our data is as folows:\n",
    "1. Scan for output files to see what benchmarks have already been run\n",
    "2. Collect data from each output file and store it into a dictionary\n",
    "3. Plot data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a way to find out what benchmarks have already been run. get_xpoints takes a directory where output data from Funwave is stored and finds which returns filenames of all the output files. We've suffixed all our output files with .out so they're easy to find. We're also sorting these points by date then run number so they'll appear in order when we plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xpoints(directory):\n",
    "    \"\"\"Create a list of x-axis points to plot based on available runs\n",
    "    Sorts by date then run number\"\"\"\n",
    "    xpoints = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if \".out\" in filename:\n",
    "            xpoints.append(filename)\n",
    "    # Sort points by date then run number\n",
    "    xpoints = sorted(xpoints, key = lambda x: (x.split(\"_\")[0], x.split(\"_\")[2]))\n",
    "    return xpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in gathering our data is to write a function to get timing results from Funwave output files. get_time_from_output takes a filename and returns a float of the simulation time if one exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes an output filename and returns a simulation time\n",
    "def get_time_from_output(output_filename):\n",
    "    \"\"\"output_filename: string of the output filename\n",
    "    returns a float of simulation time if it exists\"\"\"\n",
    "    \n",
    "    with open(output_filename, 'r') as output:\n",
    "        for line in output:\n",
    "            if \"simulation\" in line.lower():\n",
    "                line = ' '.join(line.lower().split())\n",
    "                split_line = line.split()\n",
    "                time = float(split_line[2])\n",
    "                return time\n",
    "        output.close()\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what benchmarks have been run and can also get timing data, we can store our data. gather_data uses the get_xpoints and get_time_from_output functions to store our benchmarking data into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data(directories):\n",
    "    \"\"\"Use get_time_from_output and get_xpoints to\n",
    "    create results dictionary and x ticks list\n",
    "    directories = list of directories to get data from ['np_1', 'np_2', ...]\n",
    "    results: dict of floats containing simulation times from funwave\n",
    "    ex: \n",
    "    results[1] = [57.45267612299358, 58.964640157995746, 57.09651633500471]\n",
    "    results[2] = [16.213947223004652, 16.57105119198968, 15.723207671995624]\n",
    "    results['xpoints'] = ['day1_commit1_run#', 'day2_commit2_run#', 'day3_commit2_run#']\n",
    "    \"\"\"  \n",
    "    \n",
    "    results = {}\n",
    "    results['xpoints'] = get_xpoints(directories[0])\n",
    "    \n",
    "    for directory in directories:\n",
    "        np = int(float(directory.split('/')[-1][-1]))\n",
    "        results[np] = []\n",
    "        for filename in results['xpoints']:\n",
    "            output_file = directory + '/' + filename\n",
    "            results[np].append(get_time_from_output(output_file))\n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've implemented two plotting functions for our strong scaling study. The first function plots strong scaling results for a single commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_funwave_single_commit(results, date_commit_run):\n",
    "    \"\"\"Make a strong scaling plot from a single run\n",
    "    results: dict of floats containing simulation times from funwave\n",
    "    ex: \n",
    "    results[1] = [57.45267612299358, 58.964640157995746, 57.09651633500471]\n",
    "    results[2] = [16.213947223004652, 16.57105119198968, 15.723207671995624]\n",
    "    results['xpoints'] = ['day1_commit1_run#', 'day2_commit2_run#', 'day3_commit2_run#']\n",
    "    The lists in results should be the same length!\n",
    "    \"\"\"\n",
    "    \n",
    "    np_list = [1, 2, 4]\n",
    "    index = results['xpoints'].index(date_commit_run + '.out')\n",
    "    timings = []\n",
    "    \n",
    "    for np in np_list:\n",
    "        timings.append(results[np][index])\n",
    "        \n",
    "    plt.plot(np_list, timings, marker='o', markersize=12, linewidth=2)\n",
    "    plt.title(\"Funwave Strong Scaling for {date_commit}\".format(date_commit=date_commit_run))\n",
    "    plt.ylabel('Total Simulation Time (s)')\n",
    "    plt.xlabel('NP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can track the performace of our application over time with plot_funwave_over_time. The x-axis of this plot is each unique run of our benchmark. The y-axis is the total simulation time. We structured the data in the dictionary such that creating a plot over time doesn't require much data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_funwave_over_time(results):\n",
    "    \"\"\"Plots strong scaling results over time\n",
    "    results: dict of floats containing simulation times from funwave\n",
    "    ex: \n",
    "    results[1] = [57.45267612299358, 58.964640157995746, 57.09651633500471]\n",
    "    results[2] = [16.213947223004652, 16.57105119198968, 15.723207671995624]\n",
    "    results['xpoints'] = ['day1_commit1_run#', 'day2_commit2_run#', 'day3_commit2_run#']\n",
    "    The lists in results should be the same length!\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Format x labesl\n",
    "    x_labels = [x.split('.')[0] for x in results['xpoints']]\n",
    "    for np in [1, 2, 4]:\n",
    "        plt.plot( x_labels, results[np], marker='o', markersize=12, linewidth=2, label=np)\n",
    "\n",
    "    plt.title('Funwave Strong Scaling Over Time')\n",
    "    plt.ylabel('Total Simulation Time (s)')\n",
    "    plt.xlabel('Date_Commit_Run#')\n",
    "    plt.xticks(rotation=30, horizontalalignment='right')\n",
    "    plt.legend(loc='upper left', title=\"NP\", bbox_to_anchor=(1,1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting the visualization together\n",
    "We can now plot our results in a few steps:\n",
    "* Create a list of directories that the 'gather_data' function needs find the data. \n",
    "* Plot the data with 'plot_funwave'. plot_funwave takes the dictionary returned by 'gather_data' as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = ['/home/jovyan/notebooks/build/' + np for np in ['np_1', 'np_2', 'np_4']]\n",
    "results = gather_data(directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_funwave_over_time(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_funwave_single_commit(results, '2018-10-23_d789c5d_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Benchmark\n",
    "* Let's add a simple benchmark to validate performance after each build.\n",
    "* We'll make a new feature branch for this benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh sandbox \"cd ~/FUNWAVE-TVD && git checkout -b benchmark\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to need to upload new input files in order to run a strong scaling study. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setvar(\"\"\"\n",
    "BUILD_DIR=/home/jovyan/FUNWAVE-TVD/build\n",
    "\"\"\")\n",
    "\n",
    "!files-mkdir -S ${AGAVE_STORAGE_SYSTEM_ID} -N ${BUILD_DIR}/np_1\n",
    "!files-mkdir -S ${AGAVE_STORAGE_SYSTEM_ID} -N ${BUILD_DIR}/np_2\n",
    "!files-mkdir -S ${AGAVE_STORAGE_SYSTEM_ID} -N ${BUILD_DIR}/np_4\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setvar(\"\"\"\n",
    "INPUTS_DIR=/home/jovyan/notebooks/build\n",
    "\"\"\")\n",
    "\n",
    "!files-upload -F ${INPUTS_DIR}/np_1/input.txt -S ${AGAVE_STORAGE_SYSTEM_ID} ${BUILD_DIR}/np_1/\n",
    "!files-upload -F ${INPUTS_DIR}/np_2/input.txt -S ${AGAVE_STORAGE_SYSTEM_ID} ${BUILD_DIR}/np_2/\n",
    "!files-upload -F ${INPUTS_DIR}/np_4/input.txt -S ${AGAVE_STORAGE_SYSTEM_ID} ${BUILD_DIR}/np_4/\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to notebook 9, we'll make wrapper and app scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_content = \"\"\"\n",
    "VERSION=$(cat version.txt | paste -sd \"..-\" -)\n",
    "COMMIT=$(cat commit.txt)\n",
    "\n",
    "git clone /home/jovyan/FUNWAVE-TVD\n",
    "cd FUNWAVE-TVD\n",
    "git checkout $COMMIT\n",
    "\n",
    "export BENCH_DIR=$PWD/build\n",
    "\n",
    "for np in {1,2,4}; do\n",
    "  cd ${BENCH_DIR}/np_${np}\n",
    "  echo \"Running case with np=$np\"\n",
    "  sudo docker run funwave-tvd:${VERSION} /bin/bash -c \"cp /home/install/FUNWAVE-TVD/build/np_${np}/input.txt . && mpirun -np ${np} /home/install/FUNWAVE-TVD/src/funwave_vessel\" 2>&1 | tee fwv_${np}.out\n",
    "done\n",
    "\"\"\"\n",
    "\n",
    "with open('funwave-benchmark-wrapper.txt', 'w') as benchmark_wrapper:\n",
    "    benchmark_wrapper.write(script_content)\n",
    "\n",
    "\n",
    "!files-upload -S ${AGAVE_STORAGE_SYSTEM_ID} -F funwave-benchmark-wrapper.txt /home/jovyan/FUNWAVE-TVD/build/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writefile(\"funwave-benchmark-app.txt\",\"\"\"\n",
    "{  \n",
    "   \"name\":\"${AGAVE_USERNAME}-${MACHINE_NAME}-funwave-dbenchmark\",\n",
    "   \"version\":\"3.2.0\",\n",
    "   \"label\":\"Benchmarks the funwave docker image\",\n",
    "   \"shortDescription\":\"Funwave docker benchmark\",\n",
    "   \"longDescription\":\"\",\n",
    "   \"deploymentSystem\":\"${AGAVE_STORAGE_SYSTEM_ID}\",\n",
    "   \"deploymentPath\":\"/home/jovyan/funwave-jenkins-benchmark/\",\n",
    "   \"templatePath\":\"funwave-benchmark-wrapper.txt\",\n",
    "   \"testPath\":\"version.txt\",\n",
    "   \"executionSystem\":\"${AGAVE_EXECUTION_SYSTEM_ID}\",\n",
    "   \"executionType\":\"CLI\",\n",
    "   \"parallelism\":\"SERIAL\",\n",
    "   \"modules\":[],\n",
    "   \"inputs\":[],\n",
    "   \"parameters\":[{\n",
    "     \"id\" : \"code_version\",\n",
    "     \"value\" : {\n",
    "       \"visible\":true,\n",
    "       \"required\":true,\n",
    "       \"type\":\"string\",\n",
    "       \"order\":0,\n",
    "       \"enquote\":false,\n",
    "       \"default\":\"latest\"\n",
    "     },\n",
    "     \"details\":{\n",
    "         \"label\": \"Version of the code\",\n",
    "         \"description\": \"If true, output will be packed and compressed\",\n",
    "         \"argument\": null,\n",
    "         \"showArgument\": false,\n",
    "         \"repeatArgument\": false\n",
    "     },\n",
    "     \"semantics\":{\n",
    "         \"argument\": null,\n",
    "         \"showArgument\": false,\n",
    "         \"repeatArgument\": false\n",
    "     }\n",
    "   }],\n",
    "   \"outputs\":[]\n",
    "}\n",
    "\"\"\")\n",
    "!files-upload -S ${AGAVE_STORAGE_SYSTEM_ID} -F funwave-benchmark-app.txt /home/jovyan/FUNWAVE-TVD/build/\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commit Your Benchmark, Watch It Run\n",
    "* Let's merge our benchmark back into the `dev` branch and watch it run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh sandbox \"set -x && cd ~/FUNWAVE-TVD && git add build\"\n",
    "!ssh sandbox \"cd ~/FUNWAVE-TVD && git commit -m 'Added benchmark app.'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh sandbox \"cd ~/FUNWAVE-TVD && git checkout dev && git merge --squash benchmark\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding our new data to the plot - This only works AFTER the Jenkins finishes\n",
    "Now that we've run our benchmark, we need to pull the new data down and add it to our results. To do this we'll:\n",
    "* Get the jobid of the job we just ran\n",
    "* Pull down the files for each np run\n",
    "* Move the results to our data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setvar(\"\"\"\n",
    "# Capture the output of the job list command\n",
    "OUTPUT=$(jobs-list -l 1)\n",
    "# Parse out the job id from the output\n",
    "JOB_ID=$(echo $OUTPUT | cut -d' ' -f1)\n",
    "\"\"\")\n",
    "!jobs-output-list $JOB_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jobs-output-get ${JOB_ID} FUNWAVE-TVD/build/np_1/fwv_1.out\n",
    "!jobs-output-get ${JOB_ID} FUNWAVE-TVD/build/np_2/fwv_2.out\n",
    "!jobs-output-get ${JOB_ID} FUNWAVE-TVD/build/np_4/fwv_4.out\n",
    "!jobs-output-get ${JOB_ID} commit.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "today = datetime.date.today()\n",
    "commit = \"\"\n",
    "with open('commit.txt','r') as f:\n",
    "    for ii, line in enumerate(f):\n",
    "        if ii == 0:\n",
    "            commit = line.strip()\n",
    "\n",
    "def get_outputfile_name():\n",
    "    today = datetime.date.today().strftime('%Y-%m-%d')\n",
    "    \n",
    "    commit = \"\"\n",
    "    with open('commit.txt','r') as f:\n",
    "        for ii, line in enumerate(f):\n",
    "            if ii == 0:\n",
    "                commit = line.strip()\n",
    "                \n",
    "    run_number = 1\n",
    "    while True:\n",
    "        directory = '/home/jovyan/notebooks/build/np_1/'\n",
    "        outputfile = today+'_'+commit+'_'+str(run_number)+'.out'\n",
    "        if os.path.isfile(directory + outputfile):\n",
    "            run_number += 1\n",
    "        else:\n",
    "            return outputfile\n",
    "        if run_number > 100:\n",
    "            break\n",
    "            \n",
    "    return None\n",
    "\n",
    "outputfile = get_outputfile_name()\n",
    "\n",
    "for np in ['1', '2', '4']:\n",
    "    filename = 'fwv_'+np+'.out'\n",
    "    directory = '/home/jovyan/notebooks/build/np_' + np + '/'\n",
    "    print(directory + outputfile)\n",
    "    shutil.copyfile(filename, directory + outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's plot our latest result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = ['/home/jovyan/notebooks/build/' + np for np in ['np_1', 'np_2', 'np_4']]\n",
    "results = gather_data(directories)\n",
    "plot_funwave_over_time(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's left?\n",
    "* Automating our plotting process\n",
    "\n",
    "The plotting process shown above can be added to Jenkins if desired. Ideally, these functions would be added to *Jenkins shared libraries* so that our pipeline is readable and steps can be easily swapped out or modified. \n",
    "\n",
    "* Making the benchmarks easily reproducible\n",
    "\n",
    "The benchmarks shown above are close to reproducible. However, we can still add metadata make them more meanigful. We've left out is *where* the benchmarks were run and the input file used. Including an input file directly or a way to find the input file will allow others to run the same case. Including hardware details allows others to verify performance, and gives an estimate for the performance they could expect on a their hardware.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
