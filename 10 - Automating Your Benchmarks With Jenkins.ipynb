{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/agave/funwave-tvd-jenkins-pipeline\n",
    "\n",
    "%cd ~/agave/funwave-tvd-jenkins-pipeline\n",
    "\n",
    "!pip3 install --upgrade setvar\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "from setvar import *\n",
    "\n",
    "!auth-tokens-refresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating Your Build\n",
    "* Automation is great, but things can quickly go wrong.\n",
    "* This is why CI/CD strongly emphasizes good testing practices.\n",
    "* Testing is central to CI/CD\n",
    "  * It allows you assess viability of each commit\n",
    "  * It is the determination of whether or not code can be successfully integrated\n",
    "  * It allows for code to be automatically deployed to prod, without the direct oversight of a tightly controlled group of developers.\n",
    "* Production pipelines have multiple testing guards\n",
    "  * Types of tests: Unit, Functional, Acceptance, Benchmarks, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Benchmark\n",
    "* Let's add a simple benchmark to validate performance after each build.\n",
    "* We'll make a new feature branch for this benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh sandbox \"cd ~/FUNWAVE-TVD && git checkout -b benchmark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writefile(\"funwave-benchmark-wrapper.txt\",\"\"\"\n",
    "\n",
    "VERSION=$(cat version.txt | paste -sd \".\" -)\n",
    "\n",
    "cd build/benchmark-inputs\n",
    "docker run funwave-tvd:\\${VERSION} <benchmark-command-goes-here>\n",
    "\"\"\")\n",
    "!files-upload -S ${AGAVE_STORAGE_SYSTEM_ID} -F funwave-benchmark-wrapper.txt /home/jovyan/FUNWAVE-TVD/build/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writefile(\"funwave-benchmark-app.txt\",\"\"\"\n",
    "{  \n",
    "   \"name\":\"${AGAVE_USERNAME}-${MACHINE_NAME}-funwave-dbuild\",\n",
    "   \"version\":\"1.0\",\n",
    "   \"label\":\"Benchmarks the funwave docker image\",\n",
    "   \"shortDescription\":\"Funwave docker benchmark\",\n",
    "   \"longDescription\":\"\",\n",
    "   \"deploymentSystem\":\"${AGAVE_STORAGE_SYSTEM_ID}\",\n",
    "   \"deploymentPath\":\"automation/funwave-tvd-docker-automation\",\n",
    "   \"templatePath\":\"build/funwave-benchmark-wrapper.txt\",\n",
    "   \"testPath\":\"test.txt\",\n",
    "   \"executionSystem\":\"${AGAVE_EXECUTION_SYSTEM_ID}\",\n",
    "   \"executionType\":\"CLI\",\n",
    "   \"parallelism\":\"SERIAL\",\n",
    "   \"modules\":[],\n",
    "   \"inputs\":[],\n",
    "   \"parameters\":[{\n",
    "     \"id\" : \"code_version\",\n",
    "     \"value\" : {\n",
    "       \"visible\":true,\n",
    "       \"required\":true,\n",
    "       \"type\":\"string\",\n",
    "       \"order\":0,\n",
    "       \"enquote\":false,\n",
    "       \"default\":\"latest\"\n",
    "     },\n",
    "     \"details\":{\n",
    "         \"label\": \"Version of the code\",\n",
    "         \"description\": \"If true, output will be packed and compressed\",\n",
    "         \"argument\": null,\n",
    "         \"showArgument\": false,\n",
    "         \"repeatArgument\": false\n",
    "     },\n",
    "     \"semantics\":{\n",
    "         \"argument\": null,\n",
    "         \"showArgument\": false,\n",
    "         \"repeatArgument\": false\n",
    "     }\n",
    "   }],\n",
    "   \"outputs\":[]\n",
    "}\n",
    "\"\"\")\n",
    "!files-upload -S ${AGAVE_STORAGE_SYSTEM_ID} -F funwave-benchmark-app.txt /home/jovyan/FUNWAVE-TVD/build/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automating Our Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add A Jenkinsfile to Run the Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!files-upload -S ${AGAVE_STORAGE_SYSTEM_ID} -F /home/jovyan/notebooks/build/Jenkinsfile-benchmark /home/jovyan/FUNWAVE-TVD/build/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRYing Out Our Jenkinsfiles\n",
    "Both our build and benchmark Jenkinsfiles are following the same basic workflow _(checkout code, setup Agave CLI, build and submit job to execution environment)_ which has resulted in a bunch of duplicated code that varies only by a keyword. For the sake of maintainability, we should find a way to improve the orthogonality of our pipeline.\n",
    "\n",
    "## Jenkins Shared Libraries\n",
    "A [Jenkins Shared Library](https://jenkins.io/doc/book/pipeline/shared-libraries/) is a repository of code that is centrally managed, and made accessible to multiple pipelines. These pipelines can call shared libraries as functions, and pass parameters to specify build options.\n",
    "\n",
    "## Update the Build Jenkinsfile to Trigger the Benchmark\n",
    "* We need to add a new stage to the Jenkins pipeline to run the benchmark if the preceeding stages of the build executed successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writefile(\"Jenkinsfile\", \"\"\"import groovy.json.JsonOutput\n",
    "\n",
    "def JobStages = load(\"JobStages.groovy\")\n",
    "\n",
    "\n",
    "node {\n",
    "  currentBuild.result = \"SUCCESS\"\n",
    "\n",
    "  env.AGAVE_TENANTS_API_BASEURL = \"https://sandbox.agaveplatform.org/tenants\"\n",
    "  env.MACHINE_NAME = \"sandbox\"\n",
    "  env.AGAVE_APP_NAME = \"funwave-tvd-build-\\${env.AGAVE_USERNAME}\"\n",
    "  env.AGAVE_CLIENT_NAME = \"jenkins-cli-\\${env.AGAVE_USERNAME}\"\n",
    "\n",
    "  try {\n",
    "\n",
    "    JobStages.setUpCli(env)\n",
    "    JobStages.cloneRepository()\n",
    "    JobStages.updateApp('build', env)\n",
    "    JobStages.submitJob('build', env)\n",
    "\n",
    "    stage(\"Run benchmarks\") {\n",
    "      // Benchmarking stuff goes here\n",
    "    }\n",
    "\n",
    "  } catch (error) {\n",
    "\n",
    "    currentBuild.result = \"FAILURE\"\n",
    "    throw error\n",
    "\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "!files-upload -S ${AGAVE_STORAGE_SYSTEM_ID} -F Jenkinsfile /home/jovyan/FUNWAVE-TVD/build/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commit Your Benchmark, Watch It Run\n",
    "* Let's merge our benchmark back into the `dev` branch and watch it run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh sandbox \"set -x && cd ~/FUNWAVE-TVD && git add build\"\n",
    "!ssh sandbox \"cd ~/FUNWAVE-TVD && git commit -m 'Added benchmark app and pipeline.'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh sandbox \"cd ~/FUNWAVE-TVD && git checkout dev && git merge --squash benchmark\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Our Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
