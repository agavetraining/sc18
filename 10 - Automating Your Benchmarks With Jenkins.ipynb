{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/agave/funwave-tvd-jenkins-pipeline\n",
    "\n",
    "%cd ~/agave/funwave-tvd-jenkins-pipeline\n",
    "\n",
    "!pip3 install --upgrade setvar\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "from setvar import *\n",
    "\n",
    "# This cell enables inline plotting in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!auth-tokens-refresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo ${AGAVE_STORAGE_SYSTEM_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating Your Build\n",
    "* Automation is great, but things can quickly go wrong.\n",
    "* This is why CI/CD strongly emphasizes good testing practices.\n",
    "* Testing is central to CI/CD\n",
    "  * It allows you assess viability of each commit\n",
    "  * It is the determination of whether or not code can be successfully integrated\n",
    "  * It allows for code to be automatically deployed to prod, without the direct oversight of a tightly controlled group of developers.\n",
    "* Production pipelines have multiple testing guards\n",
    "  * Types of tests: Unit, Functional, Acceptance, Benchmarks, ...\n",
    "\n",
    "In this notebook, we'll be validating the performance of Funwave by running a strong scaling study. We've already collected a few points for a strong scaling study with processor counts of 1, 2, and 4. We're going to visualize our benchmark with Matplotlib, then we'll run another Benchmark with Jenkins to add additional points, and finally we'll plot our benchmarking results including the lastest data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Initial Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've stored output files from FUNWAVE in /home/jovyan/notebooks/build/np_{1, 2, 4}. The output files are names DATE_COMMIT_RUN.out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/jovyan/notebooks/build/np_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting data\n",
    "The general flow for gathering and plotting our data is as folows:\n",
    "1. Scan for output files to see what benchmarks have already been run\n",
    "2. Collect data from each output file and store it into a dictionary\n",
    "3. Plot data from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a way to find out what benchmarks have already been run. get_xpoints takes a directory where output data from Funwave is stored and finds which returns filenames of all the output files. We've suffixed all our output files with .out so they're easy to find. We're also sorting these points by date then run number so they'll appear in order when we plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xpoints(directory):\n",
    "    \"\"\"Create a list of x-axis points to plot based on available runs\n",
    "    Sorts by date then run number\"\"\"\n",
    "    xpoints = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if \".out\" in filename:\n",
    "            xpoints.append(filename)\n",
    "    # Sort points by date then run number\n",
    "    xpoints = sorted(xpoints, key = lambda x: (x.split(\"_\")[0], x.split(\"_\")[2]))\n",
    "    return xpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in gathering our data is to write a function to get timing results from Funwave output files. get_time_from_output takes a filename and returns a float of the simulation time if one exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes an output filename and returns a simulation time\n",
    "def get_time_from_output(output_filename):\n",
    "    \"\"\"output_filename: string of the output filename\n",
    "    returns a float of simulation time if it exists\"\"\"\n",
    "    \n",
    "    with open(output_filename, 'r') as output:\n",
    "        for line in output:\n",
    "            if \"simulation\" in line.lower():\n",
    "                line = ' '.join(line.lower().split())\n",
    "                split_line = line.split()\n",
    "                time = float(split_line[2])\n",
    "                return time\n",
    "        output.close()\n",
    "    return \"No timing result found!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata_from_output(output):\n",
    "    \"\"\"output: list output from funwave split by lines\n",
    "    ex: [\"line1\", \"line2\", ...]\n",
    "    returns the date and commit of the run\"\"\"\n",
    "    date = ''\n",
    "    commit = ''\n",
    "    for line in output:\n",
    "        if \"funwave run date\" in line.lower():\n",
    "            line = ' '.join(line.lower().split())\n",
    "            split_line = line.split()\n",
    "            date = split_line[-1]\n",
    "        if \"funwave commit hash\" in line.lower():\n",
    "            line = ' '.join(line.lower().split())\n",
    "            split_line = line.split()\n",
    "            commit = split_line[-1]\n",
    "    return date + '_' + commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what benchmarks have been run and can also get timing data, we can store our data. gather_data uses the get_xpoints and get_time_from_output functions to store our benchmarking data into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data(directories):\n",
    "    \"\"\"Use get_time_from_output and get_xpoints to\n",
    "    create results dictionary and x ticks list\n",
    "    directories = list of directories to get data from ['np_1', 'np_2', ...]\n",
    "    results: dict of floats containing simulation times from funwave\n",
    "    ex: \n",
    "    results[1] = [57.45267612299358, 58.964640157995746, 57.09651633500471]\n",
    "    results[2] = [16.213947223004652, 16.57105119198968, 15.723207671995624]\n",
    "    results['xpoints'] = ['day1_commit1_run#', 'day2_commit2_run#', 'day3_commit2_run#']\n",
    "    \"\"\"  \n",
    "    \n",
    "    results = {}\n",
    "    results['xpoints'] = get_xpoints(directories[0])\n",
    "    \n",
    "    for directory in directories:\n",
    "        np = int(float(directory.split('/')[-1][-1]))\n",
    "        results[np] = []\n",
    "        for filename in results['xpoints']:\n",
    "            output_file = directory + '/' + filename\n",
    "            results[np].append(get_time_from_output(output_file))\n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've implemented two plotting functions for our strong scaling study. The first function plots strong scaling results for a single commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_funwave_single_commit(results, date_commit_run):\n",
    "    \"\"\"Make a strong scaling plot from a single run\n",
    "    results: dict of floats containing simulation times from funwave\n",
    "    ex: \n",
    "    results[1] = [57.45267612299358, 58.964640157995746, 57.09651633500471]\n",
    "    results[2] = [16.213947223004652, 16.57105119198968, 15.723207671995624]\n",
    "    results['xpoints'] = ['day1_commit1_run#', 'day2_commit2_run#', 'day3_commit2_run#']\n",
    "    The lists in results should be the same length!\n",
    "    \"\"\"\n",
    "    \n",
    "    np_list = [1, 2, 4]\n",
    "    index = results['xpoints'].index(date_commit_run + '.out')\n",
    "    timings = []\n",
    "    \n",
    "    for np in np_list:\n",
    "        timings.append(results[np][index])\n",
    "        \n",
    "    plt.plot(np_list, timings, marker='o', markersize=12, linewidth=2)\n",
    "    plt.title(\"Funwave Strong Scaling for {date_commit}\".format(date_commit=date_commit_run))\n",
    "    plt.ylabel('Total Simulation Time (s)')\n",
    "    plt.xlabel('NP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can track the performace of our application over time with plot_funwave_over_time. The x-axis of this plot is each unique run of our benchmark. The y-axis is the total simulation time. We structured the data in the dictionary such that creating a plot over time doesn't require much data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_funwave_over_time(results):\n",
    "    \"\"\"Plots strong scaling results over time\n",
    "    results: dict of floats containing simulation times from funwave\n",
    "    ex: \n",
    "    results[1] = [57.45267612299358, 58.964640157995746, 57.09651633500471]\n",
    "    results[2] = [16.213947223004652, 16.57105119198968, 15.723207671995624]\n",
    "    results['xpoints'] = ['day1_commit1_run#', 'day2_commit2_run#', 'day3_commit2_run#']\n",
    "    The lists in results should be the same length!\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Format x labesl\n",
    "    x_labels = [x.split('.')[0] for x in results['xpoints']]\n",
    "    for np in [1, 2, 4]:\n",
    "        plt.plot( x_labels, results[np], marker='o', markersize=12, linewidth=2, label=np)\n",
    "\n",
    "    plt.title('Funwave Strong Scaling Over Time')\n",
    "    plt.ylabel('Total Simulation Time (s)')\n",
    "    plt.xlabel('Date_Commit_Run#')\n",
    "    plt.xticks(rotation=30, horizontalalignment='right')\n",
    "    plt.legend(loc='upper left', title=\"NP\", bbox_to_anchor=(1,1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting the visualization together\n",
    "We can now plot our results in a few steps:\n",
    "* Create a list of directories that the 'gather_data' function needs find the data. \n",
    "* Plot the data with 'plot_funwave'. plot_funwave takes the dictionary returned by 'gather_data' as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = ['/home/jovyan/notebooks/build/' + np for np in ['np_1', 'np_2', 'np_4']]\n",
    "results = gather_data(directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_funwave_over_time(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_funwave(results, '2018-10-23_d789c5d_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Benchmark\n",
    "* Let's add a simple benchmark to validate performance after each build.\n",
    "* We'll make a new feature branch for this benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh sandbox \"cd ~/FUNWAVE-TVD && git checkout -b benchmark\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We're going to need new input files in order to run a strong scaling study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_content = \"\"\"\n",
    "VERSION=$(cat version.txt | paste -sd \".\" -)\n",
    "\n",
    "export BASE_DIR=$PWD \n",
    "for np in {1,2,4}; do\n",
    "  cd ${BASE_DIR}/np_${np}\n",
    "  docker run funwave-tvd:\\${VERSION} mpirun -np ${np} /home/install/FUNWAVE-TVD/src/funwave_vessel\n",
    "\"\"\"\n",
    "\n",
    "with open('funwave-benchmark-wrapper.txt', 'w') as benchmark_wrapper:\n",
    "    benchmark_wrapper.write(script_content)\n",
    "\n",
    "\n",
    "!files-upload -S ${AGAVE_STORAGE_SYSTEM_ID} -F funwave-benchmark-wrapper.txt /home/jovyan/FUNWAVE-TVD/build/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writefile(\"funwave-benchmark-app.txt\",\"\"\"\n",
    "{  \n",
    "   \"name\":\"${AGAVE_USERNAME}-${MACHINE_NAME}-funwave-dbenchmark\",\n",
    "   \"version\":\"1.0\",\n",
    "   \"label\":\"Benchmarks the funwave docker image\",\n",
    "   \"shortDescription\":\"Funwave docker benchmark\",\n",
    "   \"longDescription\":\"\",\n",
    "   \"deploymentSystem\":\"${AGAVE_STORAGE_SYSTEM_ID}\",\n",
    "   \"deploymentPath\":\"/home/jovyan/FUNWAVE-TVD/\",\n",
    "   \"templatePath\":\"build/funwave-benchmark-wrapper.txt\",\n",
    "   \"testPath\":\"version.txt\",\n",
    "   \"executionSystem\":\"${AGAVE_EXECUTION_SYSTEM_ID}\",\n",
    "   \"executionType\":\"CLI\",\n",
    "   \"parallelism\":\"SERIAL\",\n",
    "   \"modules\":[],\n",
    "   \"inputs\":[],\n",
    "   \"parameters\":[{\n",
    "     \"id\" : \"code_version\",\n",
    "     \"value\" : {\n",
    "       \"visible\":true,\n",
    "       \"required\":true,\n",
    "       \"type\":\"string\",\n",
    "       \"order\":0,\n",
    "       \"enquote\":false,\n",
    "       \"default\":\"latest\"\n",
    "     },\n",
    "     \"details\":{\n",
    "         \"label\": \"Version of the code\",\n",
    "         \"description\": \"If true, output will be packed and compressed\",\n",
    "         \"argument\": null,\n",
    "         \"showArgument\": false,\n",
    "         \"repeatArgument\": false\n",
    "     },\n",
    "     \"semantics\":{\n",
    "         \"argument\": null,\n",
    "         \"showArgument\": false,\n",
    "         \"repeatArgument\": false\n",
    "     }\n",
    "   }],\n",
    "   \"outputs\":[]\n",
    "}\n",
    "\"\"\")\n",
    "!files-upload -S ${AGAVE_STORAGE_SYSTEM_ID} -F funwave-benchmark-app.txt /home/jovyan/FUNWAVE-TVD/build/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automating Our Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterized Builds\n",
    "\n",
    "## Triggering Pipelines From Other Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRYing Out Jenkinsfiles In Production\n",
    "Both our build and benchmark Jenkinsfiles are following the same basic workflow _(checkout code, setup Agave CLI, build and submit job to execution environment)_ which has resulted in a bunch of duplicated code that varies only by a keyword. For the sake of maintainability, we should find a way to improve the orthogonality of our pipeline.\n",
    "\n",
    "## Jenkins Shared Libraries\n",
    "A [Jenkins Shared Library](https://jenkins.io/doc/book/pipeline/shared-libraries/) is a repository of code that is centrally managed, and made accessible to multiple pipelines. These pipelines can call shared libraries as functions, and pass parameters to specify build options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commit Your Benchmark, Watch It Run\n",
    "* Let's merge our benchmark back into the `dev` branch and watch it run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh sandbox \"set -x && cd ~/FUNWAVE-TVD && git add build\"\n",
    "!ssh sandbox \"cd ~/FUNWAVE-TVD && git commit -m 'Added benchmark app.'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh sandbox \"cd ~/FUNWAVE-TVD && git checkout dev && git merge --squash benchmark\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding our new data to the plot\n",
    "Now that we've run our benchmark, it's time to replot our data and see what has changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = ['/home/jovyan/notebooks/build/' + np for np in ['np_1', 'np_2', 'np_4']]\n",
    "results = gather_data(directories)\n",
    "plot_funwave_over_time(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's left?\n",
    "* Automating our plotting process\n",
    "\n",
    "The plotting process shown above can be added to Jenkins if desired. Ideally, these functions would be added to *Jenkins shared libraries* so that our pipeline is readable and steps can be easily swapped out or modified. \n",
    "\n",
    "* Making the benchmarks easily reproducible\n",
    "\n",
    "The benchmarks shown above are close to reproducible. However, we can still add metadata make them more meanigful. The most important metadata we've left out is *where* the benchmarks were run. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
